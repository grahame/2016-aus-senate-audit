\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{lemma}{Lemma}

\newcommand{\VTNote}[1]{{\it(VT: #1)}}

\begin{document}

\title{Auditing Australian Senate Ballots}
\author{Michelle Blom \and  Berj Chilingirian \and  Andrew Conway \and Chris Culnane \and 
Zara Perumal \and Ron Rivest 
 \and Philip Stark \and Vanessa Teague}
\maketitle

\tableofcontents

\begin{abstract}
\VTNote{Hope alphabetical order is OK by all.  Some notes about authorship etc: I've left Grahame Bowland off due to political sensitivities but I'll ask him whether he'd rather be ack'd or coauthored.  Depending on who wants to field how many questions, I'll put a little asterisk and write ``corresponding author'' on anyone who'd like to be.  I've decided that the audience is the nice statistician who works at the Australian Electoral Commission---hence I've written a lot about what audits are, and not so much about what STV is.}
\end{abstract}

\section{Introduction}
Election auditing is well understood for plurality elections but difficult for complex voting schemes.  The Australian Senate uses the Single Transferable Vote (STV).  The data digitization is partially automated (which is a good thing), and the count itself is entirely automated.  There are many characteristics that make auditing challenging:

\begin{itemize}
\item Calculating winning margins for STV is NP-hard in general, and the parameters of Australian elections (sometimes more than 150 candidates) make exact solutions infeasible in practice.  There are not even efficient methods for reliably computing good bounds. 	
\item Since there are sometimes millions of votes in one constituency, a full hand count is infeasible.
\item In practice the margins can sometimes be remarkably small.  For example, in Western Australia in 2013 a single lost box of ballots was found to be enough to change the election outcome.  In Tasmania in 2016, the final seat was determined by a difference of 141 votes 
(a margin of 71) out of over 300,000.	
\end{itemize}

This makes it difficult to fit in with existing methods for risk limiting audits. If the detected error rate is large enough
that a proper RLA (or Bayesian audit) would resort to a full recount, it's not clear what to do.  

Although the Senate digitisation and counting process has been partially automated before, recent changes to the voting rules mean that an undetected error in the electronic processing has a much greater potential to affect an outcome than before.  \VTNote{I used to think that was true.  Now that I've seen the Helen Kroger example below, I'm not so sure.  Perhaps we always should have been doing this sort of audit.}

To get an idea of the fiendish complexity of Australian Senate outcomes, consider the case of the last seat allocated to the State of Victoria in 2013.  Ricky Muir from the Australian Motoring Enthusiasts Party won the seat, in a surprise result that ousted sitting Senator 
Helen Kroger of the Liberal party.  In the last elimination round (round 291), Muir had 51,758 more votes than Kroger, and this was generally reported in the media as the margin by which he won.  However, the true margin was only 2592 (about 0.1\%).  
%Her problem was that she was campaigning for the wrong person and got too many votes herself. 
If Kroger had persuaded 1294 of her voters, and 1301 of Janet Rice (Greens)’s voters, to  vote instead for Joe Zammit (Australian Fishing and Lifestyle Party), this would have prevented Zammit from being excluded in count 224. Muir, deprived of Zammit's preferences, would have been excluded in the next count, and Kroger would have won.   (Our algorithm for searching for these small margins is described in Section~\ref{sec:winnerElimination}.)

This change could be made by altering 2595 ballots, in each case swapping two preferences, none of them first preferences, all below the line.  First preferences are relatively well scrutinised in pollsite processes before dispatch to the central counting station.  Other preferences are not.   Also \emph{lowering} a particular candidate's preference wouldn't usually be expected to help that candidate (though we are not the first to notice STV's nonmonotonicity).  So the outcome could have been changed by swapping poorly-scrutinised preferences, half of which seemed to disadvantage the candidate they actually helped, in far fewer ballots than anyone considered to be enough.  

\subsection{Our contribution}
This paper describes two main suggested approaches to auditing the paper evidence of Australian Senate votes.  
\begin{itemize}
	\item Bayesian audits~\cite{rivest2012bayesian} 
	\item Heuristic search for small margins, followed by a ``negative'' audit adapted from \cite{stark2014verifiable}.
\end{itemize}

Both amount to looking for small, unnoticed ways to change the election outcome, and trying to detect whether there were enough errors in the digitizing step to have shifted the apparent outcome away from that (other, correct) one. We also describe a simple capped scheme---its main advantage is that it would bound in advance the total amount of auditing to be done, though its results could be inconclusive. 

This would be the first time these sort of auditing steps are
being applied, and so this year's efforts would be much more
``exploratory'' in character than ``authoritative''.  We hope to be able to perform two or more kinds of audits on the same samples.  However, we do not even know, at the time of writing, whether any audit will be permitted at all.  

The key objective is to provide evidence that the announced election outcome is right, or, if it is wrong, to find out early enough that it can be corrected by careful inspection of the paper evidence.

\subsection{Background on audits}
The process begins with the electronic data file that describes full preferences for all votes in a state.  This file implies a \emph{reported election outcome} $R$, which is a set of winning candidates which we assume to be properly computed from the preferences in the data file.  (Actually we don't have to assume---we can check by rerunning the count.)  Each line in the data file is a \emph{reported vote}---we denote them $r_1,\ldots,r_n$, where $n$ is the total number of voters in the state.  Each reported vote $r_i$ corresponds to an \emph{actual vote} $a_i$ expressed on paper, which can be retrieved to check whether it matches $r_i$\footnote{\VTNote{What about omissions?}}.   The whole collection of actual votes implies an \emph{actual election outcome} $A$.  We want to know whether $A = R$.

Although the security of paper ballot processing is important, it's independent of the audit we describe here.  An audit checks whether the electronic result accurately reflects the paper evidence.  Of course if the paper evidence wasn't properly secured, that won't be detected by this process.  Our definition of ``correct'' is ``matching the retained paper votes.''

An election audit is an attempt to test the hypothesis ``That the reported election outcome is incorrect,'' that is, that $R \neq A$.  This hypothesis is tested by randomly sampling the paper ballots and recording either what election outcome the sample produces (a ballot-polling audit  (***Cite PHilip)) or how many discrepancies there are between the official data file and the paper evidence (a ballot-comparison audit  (***Cite Philip)). There are two kinds of wrong answer: an audit may declare that the official election outcome is correct when in fact it is wrong, or it may declare that the official outcome is wrong when in fact it is correct.  The latter problem is easily solved in simpler contexts by never declaring an election outcome wrong, but instead declaring that a full manual recount is required.  The first problem, of mistakenly declaring an election outcome correct when it is not, is the main concern of this paper.

An audit is \emph{Risk-limiting} if it guarantees an upper bound on the probability of mistakenly declaring a wrong outcome correct. (**Cite Philip RLAs).  A full manual recount is risk-limiting, but prohibitively expensive in our setting.  None of the audits suggested in this paper are proven to be risk limiting, however all of them provide some way of estimating the rate of errors and hence the likelihood that the announced outcome is wrong.  In some cases, the audit may not say conclusively whether the error rate is large enough to call the election result into question.  In others, we can derive some confidence either that the announced outcome is correct or that a manual inspection of all ballots is warranted.

\subsection{Bayesian Audits} Rivest and Shen's Bayesian audit~\cite{rivest2012bayesian} should give some evidence of whether small changes to the realization of the ballots could alter the results. 

The \emph{election profile} is the list of actual ballots. The auditor doesn't know them, so must work with a a probability distribution $p$ over possible election profiles, which summarises everything the auditor belives about what the actual ballots are. 

At each round we try to answer the question, ``what is the probability of each different election outcome, if we eventually audit all the ballots?''  This is answered by simulating elections based on $p$ and measuring the frequency of each outcome.  

Each audit round has three phases:
\begin{enumerate}
\item audit some paper ballots,
\item update $p$ using Bayes' Rule,
\item sample $p$ by simulating the generation of some election profiles; measure the frequency of different outcomes.
\end{enumerate}


Like any process that uses Bayes' Rule, choosing a prior is a key part of the initialization.  The suggestion in~\cite{rivest2012bayesian} is to allow any political partisan to choose the prior that most supports their political beliefs.  When everyone (who uses Bayes' Rule properly) is satisfied that the evidence points to the accuracy of the announced result, the audit can stop.  For example, the auditors could agree to stop when 95\% of simulated election outcomes match the reported outcome.


In the Australian Senate case, we assume that there will be only one apolitical auditing team (though in future candidate-appointed scrutineers could do the calculations themselves).  Hence we suggest a prior that slightly advantages everyone who lost---if the announced outcome is correct, this probability distribution will be gradually corrected towards it.

An alternative, simpler version amounts to a bootstrap, treating the population of reported ballots as
if it is the (prior) probability distribution of ballots, and then seeing how often one gets the same result for samples drawn from that prior.  

Although these audits were designed for complex elections, there are significant challenges to adapting them to the Australian Senate.  For example, the sheer number of possible ballot types makes it challenging to generate ``random'' ones efficiently.  Running the simulations efficiently is challenging when the count itself takes some time to run.  Answers to these challenges are described in Section~\ref{sec:bayes}



\subsection{Upper bounds on the margin plus ``negative'' audits}
We have developed heuristics for searching for upper bounds on the manipulation, described in Section~\ref{sec:winnerElimination}.  The technique is an extension of Carey's winner elimination upper bound (cite Carey EVT '11).  
%The other consists of extending techniques for computing the IRV margin of victory to the last rounds of an STV count.  
We can guarantee that the solution we find is genuine, {\it i.e.} a true way to change the outcome with that number of ballots, but we can't guarantee that it is minimal.  It produces a list of alternative outcomes together with an upper bound on the number of votes that need to change to produce them.

%We can approach this from either direction. 
%Given the high error rate we expect, compared to the number of %errors required to change the outcome, 
We could conduct a ``negative audit'' in the following way.  
This produces either evidence that the error rate is too small to matter or evidence that it is large enough that it could matter.  \VTNote{Is that right?  Or does it produce either evidence that it's large enough to matter, or ``don't know''?}

Suppose there are $N$ ballots in all.
Suppose we know that the outcome could be altered by altering no more than $X$ ballots in all, provided those ballots were suitably chosen.  Suppose
we think the true ballot error rate $p$ (ballots with errors divided by total ballots, no matter how many errors each ballot has) is $q$, with $qN \gg X$; that is, we think the error rate is large enough that the outcome could easily be wrong.  Then a modest sample
of size $n$ should let us infer with high confidence that $pN > X$. 

For example, a lower 99\% confidence bound for $p$ if we find 3 ballots with errors in a sample of size 500 is about 0.0009, more than double
what's needed to account for the apparent margin of 141 votes. 3 errors in a sample size of 1000 would give a lower bound of 0.00044, which is still higher than
141/339,159, which is 0.00042.  If we did find errors at about that rate, it would be strong evidence that a full re-examination of all the paper ballots is warranted.

Section~\ref{sec:comparisonAudits} details the statistical analysis and some sharper ways of distinguishing errors that are likely to make a difference to the result.

\subsection{A simple capped scheme}
We may end up doing something much simpler this time: we
could imagine having a ``cap'' on the number of randomly-chosen 
paper ballots to be examined, and then accepting the fact that
the audit results may provide less certainty than an uncapped
audit would provide.

For example, we could take a fixed sample size of 14,000
ballots (i.e. 0.1\% of the cast ballots).  We draw that
many ballots at random and examine them all.

This gives a way to estimate the error rate, which, if it is large, would give a strong argument for larger audits in the future.

\subsubsection{Combining with Bayesian Auditing} 
We can also derive some partial confidence measures from the
given sample.  For example, you could list, for each candidate,
the precentage of the time that candidate was elected across the Bayesian
experiments.  (Each experiment starts with a small urn filled with
the 14000 ballots, plus perhaps some prior ballots, and expands
it out to a full-sized profile of 14M ballots with a polya's urn
method or equivalent.  This is for a nationwide election; for the
senate the full-size profiles are the size of each senate district.)
Depending on the computation time involved, we might run say
100 such experiments.  So, you might have a final output that says:

Joe Jones     99.1 \% \\
Bob Smith     96.2 \% \\
Lila Bean       82.1 \% \\
......
Rob Meek        2.1 \% \\ 
Sandy Slip       0.4 \%   \\
Sara Tune        0.0 \%   \\

Such results are meaningful at a human level, and show
what can be reasonably concluded from the small sample.
This allows us to have a commitment to a given
level of audit effort, rather than a commitment to a given level
of audit assurance, and then give results that say something about
the assurance obtained for that level of effort.

%Since time is very short, and the uncertainty of more rigorous audit methods is a major drawback, this alternative might be the best for this year.

\subsection{Summary}
These three different audit methods could each be conducted on the same dataset.  We would generate the sample by choosing random elements of the official preference data file, then fetching the corresponding paper ballot.  The Bayesian Audit and the simple capped scheme would then simply treat the paper ballots as the random sample.  The upper-bounds based scheme would consider the errors relative to what had been reported.

\section{A description of the Senate screen-based verification process and an explanation of why its integrity guarantees are not sufficient}
\VTNote{From Chris Culnane.  I've left this in for now so that the other authors can read it, but probably we just want to summarize the main points.}

Details of the new senate counting system are fairly scarce. What information is provided raises further questions. However, we can tell that it is not software independent \cite{rivest08:e-vote}, and that there is little to no end-to-end auditing.  Election integrity seems to rely instead on the integrity of the software. 

The audit steps that are included seem to exist to comply with the legislation, as opposed to guarantee an accurate result.  \VTNote{I think this is a really important point, though one that needs to be expanded upon in order not to sound snarky.  To put it the right way round, the legislation hasn't been adequately amended to consider how to guarantee integrity of automated scanning and counting.  So the AEC has had to implement a bunch of rules that don't really make sense any more, and they've lost sight of the more important criterion of providing evidence of an accurate election result.  In short, this is an argument for amending the legislation to require just the kind of audits we're now trying to do.}

The information for this brief analysis has been garnered from the following reports:
\begin{itemize}
\item {\tt	http://www.aec.gov.au/elections/candidates/files/counting/css-faqs.pdf}
\item {\tt	http://www.aec.gov.au/elections/candidates/files/counting/css-technical-aspects.pdf}
\item {\tt	http://www.aec.gov.au/elections/candidates/files/counting/senate-count-process-diagram.pdf}
\end{itemize}

The objective of the system is, in principle, extremely simple: capture the vote preferences from the ballot papers, and then publish \& tally them. Whilst internal audit steps are useful, an overarching audit of input to output would offer the strongest assurance, however, this does not take place. There are a number of points in the system where reconciliation is not clear, the work flow for the scanning is sufficiently complicated to provide potential avenues for mistakes and malicious behaviour. A lack of transparency in the overall approach results in an inability to determine if the potential weak points are sufficiently covered by scrutineering and reconciliation. 

\subsection{Scanning and Counting Work Flow}
The work flow has a number of troubling aspects:
\begin{itemize}
\item Why are ballots that are scanned as unmarked, and having come from an informal batch, excluded from further scrutiny? This seems to predispose informal ballots to rejection without further scrutiny.
\item The flows are not 100\% accurate – in what scenario would a ballot paper, with matching scanning and data entry, end up in AEC adjudication? 
\item Why do all flows lead to the Senate Counting System – is the senate counting system being fed informal votes, which it must then remove? If so, why are there no scrutineering steps detailed inside the AEC counting phase? Surely the informal votes are removed prior to being sent to the counting software?
\item The cryptographic signature of the metadata seems to be generated as the last step prior to transmission to the AEC. As such, it is not a guarantee that the data has not changed since scanning, it only shows it has not changed during transmission or receipt. For example, after the data is classified as formal no further scrutiny is performed. As such, the integrity of the entire count is based on the trust of that central database, and every piece of software running on that server, to not alter a vote marked as formal. If an alteration did occur it would be undetectable. 
\end{itemize}

\subsection{Transmission to AEC}
\begin{itemize}
\item Why use SFTP instead of SCP? SFTP provides an interactive session, which dependent on server settings, would allow moving, deleting, renaming of files. If all that is taking place is uploading, SCP would seem like a better choice, since it only allows file copying. There are also no details about how data received via SFTP is transferred into the Segregated Secure Network, what’s more, why is the cryptographic signature not checked prior to the data being submitted into the secure network – surely it should be checked on receipt and prior to processing on the segregated network. The EasyCount server also appears to be able to log to the System log, despite being on a segregated network?
\end{itemize}

\subsection{Comparing cryptographic signature}
Since the details of the signature scheme are not provided, or what PKI is deployed, it is difficult to assess the security of this step. Are signature files checked against a restricted list of public key certificates? Do signatures just need to have been signed by a valid key in a wider PKI? What checking is performed on the validity of incoming data, beyond the signature?



\section{Bayesian Audits and how to use them in the Australian Senate}  \label{sec:bayes}
\VTNote{Currently just copying Ron's email.}

These elections are ranked-choice ballots with many candidates (maybe 100 or
so).  So, it is impossible to explicitly represent all possible ballots in
order to explicitly give a prior probability to each.  

However, these elections do allow partial ballots, where not
all candidates are listed. (Unmentioned ones effectively rank last.)

One purpose of the prior is to make sure that for small profiles you
aren't misled by small-sample statistics.  It doesn't seem fair
to confirm an election result if ballots in favor of one candidate don't
even appear in the sample.   The prior puts a thumb on the scale in
favor of ``fairness''---ensuring that all candidates are equally and
positively represented at the start.  I think this ``fairness towards
candidates'' is more important than ``fairness towards ballot types''.

So, we are proceeding along the path of representing a prior by
having one ballot cast for each candidate as a partial ballot ({\it i.e.},
listing only that candidate).  

It does not, obviously, give equal weight to all possible ballots.
The prior effectively only has support for the ballots actually 
occurring in the sample, and for the singleton ballots.  
%There
%may be a better way of thinking about this process, and fitting
%it into the Bayesian framework may or may not be exactly correct.
%Do you have thoughts on this?

(In the end, we are just effectively computing various ``noisy''
or ``fuzzy'' versions of the drawn sample, and seeing how this
affects the election outcome computation.)

The other approach that comes to mind would be to somehow generate
new ballot types de novo as the audit proceeds, and add them to
the urn.  This is not an entirely unreasonable thought, and there
is stuff in the literature that seems quite similar to this (e.g. the
literature on stick-breaking or Chinese restaurants in the Bayesian
literature).

However, there is another reason why our adopted procedure 
of having one prior ballot per candidate is attractive: efficiency.
With our procedure, the number of distinct ballot types does
not increase during the audit.  The ``Polya's Urn process'' or
equivalent only changes the frequency of each existing ballot
type---it doesn't add new types.

This means that the ``state of the urn'' is always fully representable
as a vector of length t, where t is the number of distinct ballot
types in the sample and prior.  (If the sample has size s, then t is at most s+m
where m is the number of candidates.)

In particular, this means that we can employ the gamma-variate
trick from our paper to do the audit, replacing the Polya's Urn
computation (which takes time n, where n is the number of cast
ballots) with a computation that takes only time t (computing
one gamma-variate for each ballot type, in order to obtain the
final count for the number of ballots of that type).  This improves
running time by a factor of n/t, which can be very large (e.g.
$n=10^6$ and $t=10^3$ gives a 1000-fold speedup.)




\section{Heuristics for upper bounds on STV margins}  \label{sec:winnerElimination}
% We can use text from the RealSTVMargins paper 
\VTNote{Add a discussion on Michelle's method for finding upper bounds by looking only at the last few (20 or so) rounds.}

This section explains a way of searching for small manipulations in STV elections, then suggests how this could be included in an audit.  The result is not really a risk-limiting audit, but rather an audit that would be risk limiting if certain assumptions are true.  The assumptions will be precise but computationally intractable to check. 

The idea generalises Cary's ``winner-elimination upper bound'' to STV.  The high-level idea is simple: for each candidate $w$ who won \emph{without gaining a quota}, at each step in which some other candidate $e$ is eliminated, we calculate how many votes would have to be moved from $w$ (or some other candidates with large tallies) to $e$ (and some other candidates with small tallies) to get $W$ eliminated instead.   Clearly this changes the outcome, though we don't immediately know how.

This gives us an upper bound on the size of the manipulation necessary to change the result.  There may, of course, be smaller manipulations that are not discovered by this method.  

When a solution is found, there may be many different ways to achieve it, corresponding to several different election outcomes.  

There are two slightly different variants, each corresponding to different assumptions about the source of error.  In the first variant, we allow any sort of change to ballots.  In the second, we assume that first-preferences are recorded accurately (in the polling place, under scrutiny) and that the only successful manipulation is one that leaves the first preference tallies unchanged.  In both cases there are a number of different variants and details that matter.  Although the idea generalises to any form of STV, we have implemented it for two particularly interesting ones: the idealised version of STV we used in our computation of margins \cite{blom2015efficient}, and a precise implementation of the Australian Senate counting rules \verb|(https://github.com/SiliconEconometrics/PublicService)|.

The Australian Senate rules have a number of complicating issues, including complex rules for multiple eliminations.  For this reason, we describe the idealised version of the algorithms first, then describe how we deal with some of the complexities of the Senate rules in a later section.

\subsection{Theoretical/ideal Preliminaries}
We have an announced set $W$ of winners.  $W = \{w_1, w_2, \ldots, w_s \}$ where $s$ is the number of seats.  There are two different ways of winning: $w$ may get a quota, or $w$ may be one of only $k$ candidates to remain uneliminated at the end, when there are only $k$ seats remaining to be allocated.  Sometimes these two conditions might be met simultaneously.  Let $L \subseteq W$ be the set of candidates who win by being left uneliminated at the end.  At each step, for each winner $w \in L$, we want to shift enough votes from $w$ to get $w$ eliminated at (or before) that point.  Then we want to rerun the election and see who wins instead.

Two important things to check:
\begin{itemize}
	\item that the prior elimination rounds are not affected or, if they are, that $w$ gets eliminated, and
	\item that if we shift votes that have been used to elect a candidate, the manipulation size is counted in terms of number of paper ballots, not the sum of the weights as a result of transfers.
\end{itemize}

Let $t_i(c)$ be the tally of candidate $c$ at round $i$.

\subsubsection{Variant 1: when first preferences are allowed to change}
We try to remove $m$ votes from $w$ and share that among the low-tally candidates until all other tallies are higher than $w$'s.

For each round $i$, if candidate $e$ is eliminated at round $i$,

For each $w \in W$, set 
\begin{equation} m_i(w) = \min \{m :  \forall \text{continuing candidates }  c \neq w, 
	t_i(w) - m < t_i(c) + \delta_c  \text{ where } \Sigma_c \delta_c = m   \}  
\label{eqn:margin} \end{equation}


\begin{lemma}
If $w$ has $m_i(w)$ first-preference votes, then this will produce a valid solution.  
\end{lemma}
\begin{proof}
If the shift gets $w$ eliminated before round $i$, then that's a valid solution.  

So suppose that $w$ is still standing at round $i$.  Adding extra votes to other candidates can't have affected the elimination order before now, because it could only have increased the tallies of uneliminated candidates.  (Detail: we need to argue that we also haven't accidentally given someone a quota who wouldn't otherwise have had one.  This is true because $w$ must have less than a quota at $i$ (or it's not continuing), and the recipients have received only enough votes to bring them up to $t_i(w)$ at most.)

Also note all the transfer values are 1, so the total number of ballots is equal to the total weight of the votes.
\end{proof}

If we allow non-first preferences to change too, we need to check that the manipulation doesn't affect the elimination order before $i$, or that if it does then $w$ gets eliminated anyway.  A very similar argument applies: simply take the preference that lists $w$ and substitute a preference for $c$ instead (removing any subsequent mention of $c$).  This will affect only tallies too high to have been eliminated by round $i$ and too small to have a quota.  However, we now need to be more careful about accounting for manipulations on ballots with a transfer value less than 1.  The minimum manipulation must be counted in terms of the number of ballots, not the total weight of them.  We explore this idea more precisely below.

\subsubsection{Variant 2: when first preferences must stay constant}
The next section assumes that there is some other, independent and fixed, list of first preferences, and that any manipulation must therefore leave them unchanged.  The idea is the same, but instead of removing (first) preferences from $w$, we remove (non-first) preferences for $w$ that are currently sitting on $w$'s pile.  These preferences are shifted across to (other) low-tally candidates, until enough have been shifted to get $w$ eliminated.  It's possible that a solution might exist in Variant~1, but none in Variant~2.

If there aren't enough available preferences on $w$'s pile, we can take some preferences from candidate higher than $w$.  This could sometimes affect the earlier elimination order, so the resulting solution must be re-run through the count to check that it is valid.

Now because we're not dealing with first preferences, we need to be careful to account for whole ballot papers even when the transfer values are less than one.  We need to minimise the total number of ballots shifted, given the same restrictions on tallies expressed in Equation~\ref{eqn:margin}.


\subsection{Practicalities and optimisations for the Senate}
The above section gives simple computations for computing the bounds directly.  However, the true counting algorithm for the Australian Senate is complex and fiddly.  For example, these modifications may affect rounding and the rules for multiple eliminations in ways that are hard to test for.  Other senate practicalities:
\begin{itemize}
\item  ATL votes are harder to manipulate (before 2016, impossible. after 2016, tricky)
\item  Need to use a different technique (stuff from other people) to get anything in 2013. Binary search is not used here.
\item  Tie resolution is messy.  We solve this by just overcompensating a little, to make sure there are no ties.
\item  Transfer values make which votes  to use messy (e.g a 0.4TV from $w$ vs a 1.0TV from some high scorer). We arbitrarily (and potentially inefficiently) assume that all votes from $w$ are used in preference to other people’s, when first-preference changes are allowed, but otherwise order by transfer value.    
\item Rounding is very hard to account perfectly for, and may produce small errors.
\end{itemize}

We solve this problem by simply doing a binary search for $m$, rather than computing $m$ directly, and recomputing the election outcome on the modified data to check that it is correct. 

\subsubsection{Further optimisations}
In order to eliminate $w$, it might not be necessary to eliminate them at the round where they are numerically closest to elimination.  In other words, the description above might be correct, but we might be able to achieve the same effect with fewer manipulations, if we let some of the low-tally candidates get eliminated before $w$.  This suggests the following expanded search:

Once finding the best solution using the techniques described above, take each low-tally candidate $l$ and let $r(l)$ be the votes that $l$ has received.  Do a binary search adding between 0 and $r(l)$ votes to $l$, to see whether $w$ does indeed get eliminated (in a later round ) when $l$ receives less than $r(l)$ votes.  Reset $l$'s extra votes to the minimum, $r'(l)$ for which $w$ still gets eliminated.  Iterate through all low-tally candidates, twice.  In practice we find this significantly reduces some of the margins we found.


\VTNote{I reckon we need a table like Michelle's table, listing apparent (last round) margin, margin by changing first pref's, margin without changing first pref's, and optimized margin, for a few key races from 2013 and 2016.}<++>



\section{Ballot-level comparison audits: statistics and counting details} \label{sec:comparisonAudits}
\VTNote{Add email trail with Ron et al re 3 - 8 split.}

\subsection{Some ideas for ballot-level comparison audits of the Senate outcome}
\VTNote{Philip says probably leave this out, there being not a lot of point in an RLA if there's no lower bound on the margin and no genuine option of reverting to a full hand count.  Leaving it in for now in case the discussion of which errors to ignore turns out to be useful.}

If we have two different hypotheses about the outcome, then we can see that some discrepancies couldn't affect them.  (We can't necessarily say whether a particular discrepancy might have caused the result to change in some other way.)  An outcome is defined by a set of winners, each tagged with the method by which they won (whether by getting a quota or by being uneliminated at the last round).

Let $L_1$ and $L_2$ be the sets of candidates left uneliminated at the last rounds of hypotheses 1 and 2 resp.  If a discrepancy arises at a preference that is preceded by at least one candidate in $L_1$ and at least one candidate in $L_2$, then that discrepancy cannot affect the result.  (Note that the same doesn't apply to candidates who have won by getting a quota, because those votes are distributed.)

So a suggestion for comparison audits of the Senate would look like this:
\begin{enumerate}
\item Run the winner-elimination heuristic above to find some near misses.  Each of these can translate into at least one alternative hypothesis $h$, which can be summarised as a set who win by getting a quota and a set $L_h$ who win by remaining uneliminated in the last round, together with an upper bound $M$ on the margin between $h$ and the apparent outcome.
\item (Possibly: run a cut-down version of the techniques from \cite{blom2015efficient}, for as much of the tail of the computation as is feasible, to search for possibly improved upper bounds.  \VTNote{This comment could probably do with some expanding/explaining.  Seems worth doing, though it isn't clear exactly what it proves...  })
\item Run a risk-limiting audit with those different hypotheses as different ``candidates'' and the margins as given.  See below. 
\end{enumerate}

\VTNote{Philip, I need a little help here.  Not sure exactly what is valid.  Clearly we only ever get assurances of the form, ``if this margin is indeed minimal then P.''  I need to understand valid propositions P.   What I don't fully understand is how different errors might need to be counted towards different hypotheses.  Is it as simple as treating each one as a different ``candidate'' in MICRO, or is that not quite right because of correlated errors?  But then MICRO can have correlated errors too, right?}

\emph{Big Unproven Assumption: that these are all the possible true outcomes we have to worry about, and that in each case we've found the most efficient way of switching to that outcome.}  Note that it is easy to construct test cases in which this assumption is false.  However, it seems in practice likely enough to be true that it's worth working with, in parallel with other methods that don't make this assumption.

Slightly different alternative bookeeping: Background: call the “penultimate round” the one in which there are k seats left to fill, and k+1 candidates remaining uneliminated.  (There might not be any such round, e.g. in NT.)  We identify the one with the smallest tally and declare the other k elected.  Any error “hidden” lower on the preference list than any of these k+1 candidates won’t change anything.  However, that same error might not be hidden in the penultimate round of the true (paper-based) outcome.  So:

 

We have an apparent outcome A, in which the penultimate-round candidates are $PC(A) = a_1, \ldots, a_{k+1}$.

We have a number of alternative hypotheses H1, H2, H3,…  For example, in the Tasmanian case the alternative hypothesis is to shift 71 votes from GRN to ON.  Each alternative hypothesis has its own list of penultimate candidates (not necessarily the same length):

$PC(H1) = b_{11}, b_{12}, \ldots $


$PC(H2) = b_{21}, b_{22}\ldots $

 

So now suppose we choose an electronic ballot e and fetch its (true) paper ballot p.



- If, for all discrepancies between e and p, for all hypotheses h (including A), there is an element of PC(h) higher in the preference list than the discrepancy, count it as “hidden”.  (This error can’t make a difference between A and H1, H2,… because it’s always hidden, even at the penultimate round.)

- Call the “most-preferred PC (MPPC)” the highest-ranked candidate in $ PC(A) \cup PC(H1) \cup PC(H2),\ldots$ If there’s no such candidate, call it $\bot$.  If there is someone, this is the candidate who benefits from this vote in a penultimate round.

If MPPC(p) is different from MPPC(e), count it as “important”.  This error could certainly make a difference between A, H1, H2, …

We might need to think about whether to count only those errors that take us \emph{away} from A towards an alternative hypothesis, e.g. if MPPC(e) is in PC(A) but MPPC(p) isn’t. 

(Andrew points out that the disappearance of anything in PC(A) should also count.)

- If neither of the above, count it as “maybe”.  We don’t know whether it might make a difference.  These would be perhaps the sort of errors that mess up an early stage in the elimination, but we don’t know whether they might ultimately cascade into a different election outcome.

\VTNote{Note that all this was written before the Helen Kroger example.  Perhaps we should just count everything---between subtle tweaks and nonmonotonicity, anything could count.}

\subsubsection{Idea 1: Similar to ``super-simple'' RLA's}
Use the main ideas of \cite{stark2010super}, though we only have one race.  Whenever you see a discrepancy, count it as if it contributes to the minimum margin.

This could possibly be made more efficient by ignoring some discrepancies, if we can be confident they don't make a difference.
Let $H$ be the set of all identified alternative hypotheses, and let $a$ be the apparent outcome.  As above, let $L_h$ be the set of candidates that remain uneliminated at the end of the count.  The following discrepancies can be ignored:
\begin{itemize}
\item If for any hypothesis $h \in H \cup \{a \}$, there is at least one candidate in $L_h$ appearing on that ballot in an earlier position than the discrepancy.
\item \VTNote{There must be others.  Unfortunately, it's not even clear that you can ignore discrepancies that seem to advantage someone who has already won, because they might mess up the elimination order and hence have bizarre unpredictable effects.  STV is not monotonic.}
\end{itemize}



\subsubsection{Idea 2: Use MICRO}
The idea here \VTNote{ and I'm not sure whether it's valid} is to consider each alternative hypothesis as a ``candidate'' and its corresponding margin as the ``margin'' by which that ``candidate'' missed out.  Then use MICRO \cite{stark2008sharper}.  We might sometimes be able to ignore a certain discrepany when considering one alternative $h$, but not for all of them.  (For instance, if the discrepany appears after candidates in $L_h$ and $L_a$, but not necessarily after candidates in every other $L_{h'}$.)  

Of course, we'd end up counting a very large number of discrepancies against every margin---I assume that in that case we just count them as if they contribute to the smallest.  This might not be much different from Idea~1. 

\bibliographystyle{alpha}
\bibliography{e-vote}

\end{document}
